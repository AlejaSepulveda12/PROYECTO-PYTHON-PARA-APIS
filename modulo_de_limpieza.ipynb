{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQCBWTx4hxg5"
      },
      "source": [
        "# üßπ M√≥dulo de Limpieza Avanzada de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEwbvjoXhxg9"
      },
      "source": [
        "## üì¶ Instalaci√≥n de Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0ZTywTzhxg-"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias necesarias\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn scipy -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cfJKdQThxg_"
      },
      "source": [
        "## üìö Importar Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV-C2firhxhA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh2PG1fUhxhA"
      },
      "source": [
        "# üîß PARTE 1: M√≥dulo de Limpieza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-bCh8QHhxhA"
      },
      "source": [
        "## 1.1 Clase AnalizadorDatos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU_i6J0LhxhB"
      },
      "outputs": [],
      "source": [
        "class AnalizadorDatos:\n",
        "    \"\"\"\n",
        "    Clase para identificar y analizar tipos de variables y datos faltantes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        self.reporte = {}\n",
        "\n",
        "    def identificar_tipos_variables(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Identifica tipos de variables con an√°lisis estad√≠stico detallado.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üîç AN√ÅLISIS DE TIPOS DE VARIABLES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        clasificacion = {\n",
        "            'numericas_continuas': [],\n",
        "            'numericas_discretas': [],\n",
        "            'categoricas_nominales': [],\n",
        "            'categoricas_ordinales': [],\n",
        "            'fechas': [],\n",
        "            'texto': [],\n",
        "            'booleanas': []\n",
        "        }\n",
        "\n",
        "        for col in self.df.columns:\n",
        "            tipo_detectado = self._detectar_tipo_variable(col)\n",
        "            clasificacion[tipo_detectado].append(col)\n",
        "\n",
        "        # Imprimir reporte\n",
        "        for tipo, columnas in clasificacion.items():\n",
        "            if columnas:\n",
        "                print(f\"\\nüìä {tipo.upper().replace('_', ' ')}: {len(columnas)}\")\n",
        "                for col in columnas:\n",
        "                    stats = self._estadisticas_columna(col)\n",
        "                    print(f\"   ‚Ä¢ {col}: {stats}\")\n",
        "\n",
        "        self.reporte['clasificacion'] = clasificacion\n",
        "        return clasificacion\n",
        "\n",
        "    def _detectar_tipo_variable(self, col: str) -> str:\n",
        "        \"\"\"Detecta el tipo de variable usando heur√≠sticas estad√≠sticas.\"\"\"\n",
        "        serie = self.df[col]\n",
        "\n",
        "        if self._es_fecha(serie):\n",
        "            return 'fechas'\n",
        "\n",
        "        if serie.dtype == bool or set(serie.dropna().unique()).issubset({0, 1, True, False, 'True', 'False', 'true', 'false'}):\n",
        "            return 'booleanas'\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(serie):\n",
        "            valores_unicos = serie.nunique()\n",
        "            n_total = len(serie.dropna())\n",
        "\n",
        "            if valores_unicos < 10 or valores_unicos / n_total < 0.05:\n",
        "                return 'numericas_discretas'\n",
        "            else:\n",
        "                if serie.dtype in ['int64', 'int32'] and valores_unicos < n_total * 0.5:\n",
        "                    return 'numericas_discretas'\n",
        "                return 'numericas_continuas'\n",
        "        else:\n",
        "            valores_unicos = serie.nunique()\n",
        "            if valores_unicos < 20:\n",
        "                return 'categoricas_nominales'\n",
        "            elif valores_unicos < 50:\n",
        "                return 'categoricas_ordinales'\n",
        "            else:\n",
        "                return 'texto'\n",
        "\n",
        "    def _es_fecha(self, serie: pd.Series) -> bool:\n",
        "        \"\"\"Verifica si una columna contiene fechas.\"\"\"\n",
        "        if pd.api.types.is_datetime64_any_dtype(serie):\n",
        "            return True\n",
        "        try:\n",
        "            muestra = serie.dropna().head(100)\n",
        "            if len(muestra) > 0:\n",
        "                pd.to_datetime(muestra, errors='coerce')\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "    def _estadisticas_columna(self, col: str) -> str:\n",
        "        \"\"\"Genera estad√≠sticas descriptivas de una columna.\"\"\"\n",
        "        serie = self.df[col]\n",
        "        n_valores = len(serie)\n",
        "        n_nulos = serie.isna().sum()\n",
        "        n_unicos = serie.nunique()\n",
        "        pct_nulos = (n_nulos / n_valores) * 100\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(serie):\n",
        "            return f\"√önicos={n_unicos}, Nulos={n_nulos} ({pct_nulos:.1f}%), Rango=[{serie.min():.2f}, {serie.max():.2f}]\"\n",
        "        else:\n",
        "            return f\"√önicos={n_unicos}, Nulos={n_nulos} ({pct_nulos:.1f}%)\"\n",
        "\n",
        "    def analizar_datos_faltantes(self) -> pd.DataFrame:\n",
        "        \"\"\"Analiza patrones de datos faltantes usando m√©todos estad√≠sticos.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìâ AN√ÅLISIS DE DATOS FALTANTES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        analisis = []\n",
        "\n",
        "        for col in self.df.columns:\n",
        "            n_nulos = self.df[col].isna().sum()\n",
        "            pct_nulos = (n_nulos / len(self.df)) * 100\n",
        "\n",
        "            if n_nulos > 0:\n",
        "                patron = self._detectar_patron_missingness(col)\n",
        "\n",
        "                analisis.append({\n",
        "                    'columna': col,\n",
        "                    'n_nulos': n_nulos,\n",
        "                    'pct_nulos': pct_nulos,\n",
        "                    'patron': patron,\n",
        "                    'tipo': self._detectar_tipo_variable(col)\n",
        "                })\n",
        "\n",
        "        df_analisis = pd.DataFrame(analisis).sort_values('pct_nulos', ascending=False)\n",
        "\n",
        "        if len(df_analisis) > 0:\n",
        "            print(\"\\nüìã Resumen de Datos Faltantes:\")\n",
        "            print(df_analisis.to_string(index=False))\n",
        "            print(f\"\\nüî¨ Patr√≥n de Missingness: {self._test_mcar()}\")\n",
        "        else:\n",
        "            print(\"\\n‚úÖ No se detectaron datos faltantes\")\n",
        "\n",
        "        self.reporte['datos_faltantes'] = df_analisis\n",
        "        return df_analisis\n",
        "\n",
        "    def _detectar_patron_missingness(self, col: str) -> str:\n",
        "        \"\"\"Detecta el patr√≥n de datos faltantes (MCAR, MAR, MNAR).\"\"\"\n",
        "        mascara_nulos = self.df[col].isna()\n",
        "        correlaciones = []\n",
        "\n",
        "        for otra_col in self.df.columns:\n",
        "            if otra_col != col and pd.api.types.is_numeric_dtype(self.df[otra_col]):\n",
        "                corr = np.corrcoef(mascara_nulos, self.df[otra_col].fillna(0))[0, 1]\n",
        "                if abs(corr) > 0.3:\n",
        "                    correlaciones.append((otra_col, corr))\n",
        "\n",
        "        if len(correlaciones) > 0:\n",
        "            return \"MAR (posiblemente)\"\n",
        "        else:\n",
        "            return \"MCAR (posiblemente)\"\n",
        "\n",
        "    def _test_mcar(self) -> str:\n",
        "        \"\"\"Test simplificado de Little para MCAR.\"\"\"\n",
        "        nulos_por_fila = self.df.isna().sum(axis=1)\n",
        "        varianza = nulos_por_fila.var()\n",
        "\n",
        "        if varianza < 1:\n",
        "            return \"Probable (baja varianza en distribuci√≥n de nulos)\"\n",
        "        else:\n",
        "            return \"Improbable (alta varianza sugiere patr√≥n estructurado)\"\n",
        "\n",
        "    def generar_reporte_completo(self) -> Dict:\n",
        "        \"\"\"Genera un reporte completo del an√°lisis.\"\"\"\n",
        "        self.identificar_tipos_variables()\n",
        "        self.analizar_datos_faltantes()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìä RESUMEN GENERAL\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Filas: {len(self.df):,}\")\n",
        "        print(f\"Columnas: {len(self.df.columns)}\")\n",
        "        print(f\"Memoria: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "        return self.reporte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO3QFxoyhxhC"
      },
      "source": [
        "## 1.2 Clase ImputadorAvanzado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yifSVgkqhxhD"
      },
      "outputs": [],
      "source": [
        "class ImputadorAvanzado:\n",
        "    \"\"\"\n",
        "    Clase para imputaci√≥n de datos usando m√©todos estad√≠sticos avanzados.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.copy()\n",
        "        self.df_original = df.copy()\n",
        "\n",
        "    def imputar_knn(self, columnas: List[str], n_neighbors: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"Imputa valores usando K-Nearest Neighbors.\"\"\"\n",
        "        from sklearn.impute import KNNImputer\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"üîß IMPUTACI√ìN KNN (k={n_neighbors})\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        df_numerico = self.df.select_dtypes(include=[np.number])\n",
        "        imputer = KNNImputer(n_neighbors=n_neighbors, weights='distance')\n",
        "        datos_imputados = imputer.fit_transform(df_numerico)\n",
        "\n",
        "        df_resultado = self.df.copy()\n",
        "        df_resultado[df_numerico.columns] = datos_imputados\n",
        "\n",
        "        for col in columnas:\n",
        "            if col in df_numerico.columns:\n",
        "                n_imputados = self.df_original[col].isna().sum()\n",
        "                if n_imputados > 0:\n",
        "                    valor_medio_imputado = df_resultado.loc[self.df_original[col].isna(), col].mean()\n",
        "                    print(f\"‚úÖ {col}: {n_imputados} valores imputados (media imputada: {valor_medio_imputado:.2f})\")\n",
        "\n",
        "        self.df = df_resultado\n",
        "        return df_resultado\n",
        "\n",
        "    def imputar_mice(self, columnas: List[str], max_iter: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"Imputa valores usando MICE (Multivariate Imputation by Chained Equations).\"\"\"\n",
        "        from sklearn.experimental import enable_iterative_imputer\n",
        "        from sklearn.impute import IterativeImputer\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"üîß IMPUTACI√ìN MICE (max_iter={max_iter})\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        df_numerico = self.df.select_dtypes(include=[np.number])\n",
        "        imputer = IterativeImputer(max_iter=max_iter, random_state=42, verbose=0)\n",
        "        datos_imputados = imputer.fit_transform(df_numerico)\n",
        "\n",
        "        df_resultado = self.df.copy()\n",
        "        df_resultado[df_numerico.columns] = datos_imputados\n",
        "\n",
        "        for col in columnas:\n",
        "            if col in df_numerico.columns:\n",
        "                n_imputados = self.df_original[col].isna().sum()\n",
        "                if n_imputados > 0:\n",
        "                    valor_medio_imputado = df_resultado.loc[self.df_original[col].isna(), col].mean()\n",
        "                    print(f\"‚úÖ {col}: {n_imputados} valores imputados (media imputada: {valor_medio_imputado:.2f})\")\n",
        "\n",
        "        self.df = df_resultado\n",
        "        return df_resultado\n",
        "\n",
        "    def imputar_interpolacion(self, columnas: List[str], metodo: str = 'linear') -> pd.DataFrame:\n",
        "        \"\"\"Imputa valores usando interpolaci√≥n.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"üîß IMPUTACI√ìN POR INTERPOLACI√ìN ({metodo})\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        df_resultado = self.df.copy()\n",
        "\n",
        "        for col in columnas:\n",
        "            if col in df_resultado.columns:\n",
        "                n_imputados = df_resultado[col].isna().sum()\n",
        "                if n_imputados > 0:\n",
        "                    df_resultado[col] = df_resultado[col].interpolate(method=metodo, limit_direction='both')\n",
        "                    print(f\"‚úÖ {col}: {n_imputados} valores imputados por interpolaci√≥n {metodo}\")\n",
        "\n",
        "        self.df = df_resultado\n",
        "        return df_resultado\n",
        "\n",
        "    def imputar_regresion(self, columnas: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Imputa valores usando regresi√≥n lineal multivariable.\"\"\"\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üîß IMPUTACI√ìN POR REGRESI√ìN LINEAL\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        df_resultado = self.df.copy()\n",
        "\n",
        "        for col in columnas:\n",
        "            if col in df_resultado.columns and pd.api.types.is_numeric_dtype(df_resultado[col]):\n",
        "                n_imputados = df_resultado[col].isna().sum()\n",
        "\n",
        "                if n_imputados > 0:\n",
        "                    mask_completos = df_resultado[col].notna()\n",
        "                    features = df_resultado.select_dtypes(include=[np.number]).columns.drop(col)\n",
        "                    features_sin_nulos = [f for f in features if df_resultado[f].isna().sum() == 0]\n",
        "\n",
        "                    if len(features_sin_nulos) > 0:\n",
        "                        X_train = df_resultado.loc[mask_completos, features_sin_nulos]\n",
        "                        y_train = df_resultado.loc[mask_completos, col]\n",
        "                        X_pred = df_resultado.loc[~mask_completos, features_sin_nulos]\n",
        "\n",
        "                        modelo = LinearRegression()\n",
        "                        modelo.fit(X_train, y_train)\n",
        "                        y_pred = modelo.predict(X_pred)\n",
        "                        df_resultado.loc[~mask_completos, col] = y_pred\n",
        "\n",
        "                        print(f\"‚úÖ {col}: {n_imputados} valores imputados por regresi√≥n (R¬≤ = {modelo.score(X_train, y_train):.3f})\")\n",
        "                    else:\n",
        "                        print(f\"‚ö†Ô∏è {col}: No hay suficientes features para regresi√≥n\")\n",
        "\n",
        "        self.df = df_resultado\n",
        "        return df_resultado\n",
        "\n",
        "    def eliminar_categoricos_nulos(self, umbral_porcentaje: float = 50.0) -> pd.DataFrame:\n",
        "        \"\"\"Elimina filas con valores nulos en variables categ√≥ricas.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        df_resultado = self.df.copy()\n",
        "        n_inicial = len(df_resultado)\n",
        "        cols_categoricas = df_resultado.select_dtypes(include=['object', 'category']).columns\n",
        "        columnas_eliminadas = []\n",
        "\n",
        "        for col in cols_categoricas:\n",
        "            pct_nulos = (df_resultado[col].isna().sum() / len(df_resultado)) * 100\n",
        "\n",
        "            if pct_nulos > umbral_porcentaje:\n",
        "                df_resultado = df_resultado.drop(columns=[col])\n",
        "                columnas_eliminadas.append(col)\n",
        "                print(f\"üóëÔ∏è  Columna '{col}' eliminada ({pct_nulos:.1f}% nulos)\")\n",
        "            elif pct_nulos > 0:\n",
        "                n_nulos = df_resultado[col].isna().sum()\n",
        "                df_resultado = df_resultado.dropna(subset=[col])\n",
        "                print(f\"‚úÖ '{col}': {n_nulos} filas eliminadas ({pct_nulos:.1f}% nulos)\")\n",
        "\n",
        "        n_final = len(df_resultado)\n",
        "        print(f\"\\nüìä Filas totales eliminadas: {n_inicial - n_final} ({((n_inicial - n_final) / n_inicial * 100):.1f}%)\")\n",
        "\n",
        "        self.df = df_resultado\n",
        "        return df_resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiH8FbQ6hxhE"
      },
      "source": [
        "## 1.3 Clase CorreccionFormatos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoE41I0phxhF"
      },
      "outputs": [],
      "source": [
        "class CorreccionFormatos:\n",
        "    \"\"\"Clase para correcci√≥n de formatos, especialmente fechas.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.copy()\n",
        "\n",
        "    def corregir_fechas(self, columnas: Optional[List[str]] = None,\n",
        "                       formato: Optional[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Detecta y corrige formatos de fecha.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìÖ CORRECCI√ìN DE FORMATOS DE FECHA\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        df_resultado = self.df.copy()\n",
        "\n",
        "        if columnas is None:\n",
        "            columnas = []\n",
        "            for col in df_resultado.columns:\n",
        "                if self._es_posible_fecha(df_resultado[col]):\n",
        "                    columnas.append(col)\n",
        "\n",
        "        for col in columnas:\n",
        "            try:\n",
        "                if formato:\n",
        "                    df_resultado[col] = pd.to_datetime(df_resultado[col], format=formato, errors='coerce')\n",
        "                else:\n",
        "                    df_resultado[col] = pd.to_datetime(df_resultado[col], infer_datetime_format=True, errors='coerce')\n",
        "\n",
        "                n_convertidos = df_resultado[col].notna().sum()\n",
        "                n_fallidos = df_resultado[col].isna().sum()\n",
        "                print(f\"‚úÖ '{col}': {n_convertidos} fechas convertidas, {n_fallidos} fallos\")\n",
        "\n",
        "                if n_convertidos > 0:\n",
        "                    min_fecha = df_resultado[col].min()\n",
        "                    max_fecha = df_resultado[col].max()\n",
        "                    print(f\"   Rango: {min_fecha} a {max_fecha}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error en '{col}': {str(e)}\")\n",
        "\n",
        "        self.df = df_resultado\n",
        "        return df_resultado\n",
        "\n",
        "    def _es_posible_fecha(self, serie: pd.Series) -> bool:\n",
        "        \"\"\"Verifica si una columna podr√≠a contener fechas.\"\"\"\n",
        "        if pd.api.types.is_datetime64_any_dtype(serie):\n",
        "            return True\n",
        "        if pd.api.types.is_numeric_dtype(serie):\n",
        "            return False\n",
        "\n",
        "        muestra = serie.dropna().head(50)\n",
        "        if len(muestra) == 0:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            convertidos = pd.to_datetime(muestra, errors='coerce')\n",
        "            pct_exitoso = convertidos.notna().sum() / len(muestra)\n",
        "            return pct_exitoso > 0.5\n",
        "        except:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44JLdt5zhxhG"
      },
      "source": [
        "## 1.4 Clase LimpiadorCompleto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Zwc8NohxhG"
      },
      "outputs": [],
      "source": [
        "class LimpiadorCompleto:\n",
        "    \"\"\"Clase orquestadora que combina todos los m√©todos de limpieza.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.copy()\n",
        "        self.df_original = df.copy()\n",
        "        self.historial = []\n",
        "\n",
        "    def pipeline_completo(self, metodo_imputacion: str = 'knn',\n",
        "                         eliminar_nulos_categoricos: bool = True,\n",
        "                         corregir_fechas: bool = True,\n",
        "                         **kwargs) -> pd.DataFrame:\n",
        "        \"\"\"Ejecuta el pipeline completo de limpieza.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üöÄ INICIANDO PIPELINE DE LIMPIEZA COMPLETO\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        analizador = AnalizadorDatos(self.df)\n",
        "        reporte = analizador.generar_reporte_completo()\n",
        "        clasificacion = reporte['clasificacion']\n",
        "\n",
        "        if corregir_fechas and len(clasificacion['fechas']) > 0:\n",
        "            corrector = CorreccionFormatos(self.df)\n",
        "            self.df = corrector.corregir_fechas()\n",
        "            self.historial.append(\"Fechas corregidas\")\n",
        "\n",
        "        cols_numericas = clasificacion['numericas_continuas'] + clasificacion['numericas_discretas']\n",
        "        cols_con_nulos = [col for col in cols_numericas if self.df[col].isna().sum() > 0]\n",
        "\n",
        "        if len(cols_con_nulos) > 0:\n",
        "            imputador = ImputadorAvanzado(self.df)\n",
        "\n",
        "            if metodo_imputacion == 'knn':\n",
        "                n_neighbors = kwargs.get('n_neighbors', 5)\n",
        "                self.df = imputador.imputar_knn(cols_con_nulos, n_neighbors=n_neighbors)\n",
        "            elif metodo_imputacion == 'mice':\n",
        "                max_iter = kwargs.get('max_iter', 10)\n",
        "                self.df = imputador.imputar_mice(cols_con_nulos, max_iter=max_iter)\n",
        "            elif metodo_imputacion == 'interpolacion':\n",
        "                metodo = kwargs.get('metodo_interpolacion', 'linear')\n",
        "                self.df = imputador.imputar_interpolacion(cols_con_nulos, metodo=metodo)\n",
        "            elif metodo_imputacion == 'regresion':\n",
        "                self.df = imputador.imputar_regresion(cols_con_nulos)\n",
        "\n",
        "            self.historial.append(f\"Imputaci√≥n {metodo_imputacion} aplicada\")\n",
        "\n",
        "        if eliminar_nulos_categoricos:\n",
        "            imputador = ImputadorAvanzado(self.df)\n",
        "            umbral = kwargs.get('umbral_categoricos', 50.0)\n",
        "            self.df = imputador.eliminar_categoricos_nulos(umbral_porcentaje=umbral)\n",
        "            self.historial.append(\"Nulos categ√≥ricos eliminados\")\n",
        "\n",
        "        self._resumen_final()\n",
        "        return self.df\n",
        "\n",
        "    def _resumen_final(self):\n",
        "        \"\"\"Imprime resumen final de la limpieza.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìä RESUMEN FINAL DE LIMPIEZA\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nüî∏ Filas originales: {len(self.df_original):,}\")\n",
        "        print(f\"üî∏ Filas finales: {len(self.df):,}\")\n",
        "        print(f\"üî∏ Filas eliminadas: {len(self.df_original) - len(self.df):,}\")\n",
        "        print(f\"\\nüî∏ Columnas originales: {len(self.df_original.columns)}\")\n",
        "        print(f\"üî∏ Columnas finales: {len(self.df.columns)}\")\n",
        "\n",
        "        nulos_originales = self.df_original.isna().sum().sum()\n",
        "        nulos_finales = self.df.isna().sum().sum()\n",
        "        print(f\"\\nüî∏ Nulos originales: {nulos_originales:,}\")\n",
        "        print(f\"üî∏ Nulos finales: {nulos_finales:,}\")\n",
        "        print(f\"üî∏ Reducci√≥n de nulos: {((nulos_originales - nulos_finales) / max(nulos_originales, 1) * 100):.1f}%\")\n",
        "        print(\"\\n‚úÖ Pipeline de limpieza completado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5diIexzhxhH"
      },
      "source": [
        "# üìä PARTE 2: Tests Estad√≠sticos para Selecci√≥n de M√©todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JddbuOf2hxhH"
      },
      "source": [
        "## 2.1 Clase EvaluadorMetodos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO96RnakhxhH"
      },
      "outputs": [],
      "source": [
        "class EvaluadorMetodos:\n",
        "    \"\"\"\n",
        "    Clase para evaluar y comparar m√©todos de imputaci√≥n usando tests estad√≠sticos.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df_original: pd.DataFrame):\n",
        "        \"\"\"Inicializa con el dataset original (con valores completos si es posible).\"\"\"\n",
        "        self.df_original = df_original.copy()\n",
        "        self.resultados = {}\n",
        "\n",
        "    def introducir_nulos_controlados(self, columnas: List[str], porcentaje: float = 20.0) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Introduce valores nulos de forma controlada para evaluar m√©todos.\n",
        "\n",
        "        Args:\n",
        "            columnas: Columnas donde introducir nulos\n",
        "            porcentaje: Porcentaje de valores a eliminar\n",
        "        \"\"\"\n",
        "        df_con_nulos = self.df_original.copy()\n",
        "\n",
        "        np.random.seed(42)\n",
        "\n",
        "        for col in columnas:\n",
        "            n_nulos = int(len(df_con_nulos) * porcentaje / 100)\n",
        "            indices = np.random.choice(df_con_nulos.index, n_nulos, replace=False)\n",
        "            df_con_nulos.loc[indices, col] = np.nan\n",
        "\n",
        "        print(f\"\\n‚úÖ Introducidos {porcentaje}% de nulos en {len(columnas)} columnas\")\n",
        "        return df_con_nulos\n",
        "\n",
        "    def calcular_metricas_error(self, df_imputado: pd.DataFrame,\n",
        "                               columnas: List[str],\n",
        "                               mascara_nulos: pd.DataFrame) -> Dict:\n",
        "        \"\"\"\n",
        "        Calcula m√©tricas de error comparando valores imputados con originales.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con MAE, RMSE, MAPE, R¬≤\n",
        "        \"\"\"\n",
        "        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "        metricas = {}\n",
        "\n",
        "        for col in columnas:\n",
        "            # Obtener valores originales y imputados solo donde hab√≠a nulos\n",
        "            valores_originales = self.df_original.loc[mascara_nulos[col], col]\n",
        "            valores_imputados = df_imputado.loc[mascara_nulos[col], col]\n",
        "\n",
        "            # Calcular m√©tricas\n",
        "            mae = mean_absolute_error(valores_originales, valores_imputados)\n",
        "            rmse = np.sqrt(mean_squared_error(valores_originales, valores_imputados))\n",
        "\n",
        "            # MAPE (Mean Absolute Percentage Error)\n",
        "            mape = np.mean(np.abs((valores_originales - valores_imputados) / valores_originales)) * 100\n",
        "\n",
        "            # R¬≤ Score\n",
        "            r2 = r2_score(valores_originales, valores_imputados)\n",
        "\n",
        "            # Bias (sesgo)\n",
        "            bias = np.mean(valores_imputados - valores_originales)\n",
        "\n",
        "            metricas[col] = {\n",
        "                'MAE': mae,\n",
        "                'RMSE': rmse,\n",
        "                'MAPE': mape,\n",
        "                'R2': r2,\n",
        "                'Bias': bias,\n",
        "                'n_valores': len(valores_originales)\n",
        "            }\n",
        "\n",
        "        return metricas\n",
        "\n",
        "    def test_kolmogorov_smirnov(self, df_imputado: pd.DataFrame,\n",
        "                                columnas: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Test de Kolmogorov-Smirnov para comparar distribuciones.\n",
        "        H0: Las distribuciones son iguales\n",
        "        \"\"\"\n",
        "        from scipy.stats import ks_2samp\n",
        "\n",
        "        resultados_ks = {}\n",
        "\n",
        "        for col in columnas:\n",
        "            valores_originales = self.df_original[col].dropna()\n",
        "            valores_imputados = df_imputado[col].dropna()\n",
        "\n",
        "            statistic, p_value = ks_2samp(valores_originales, valores_imputados)\n",
        "\n",
        "            resultados_ks[col] = {\n",
        "                'statistic': statistic,\n",
        "                'p_value': p_value,\n",
        "                'son_similares': p_value > 0.05  # No rechazamos H0\n",
        "            }\n",
        "\n",
        "        return resultados_ks\n",
        "\n",
        "    def test_anderson_darling(self, df_imputado: pd.DataFrame,\n",
        "                             columnas: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Test de Anderson-Darling para normalidad.\n",
        "        \"\"\"\n",
        "        from scipy.stats import anderson\n",
        "\n",
        "        resultados_ad = {}\n",
        "\n",
        "        for col in columnas:\n",
        "            valores_imputados = df_imputado[col].dropna()\n",
        "\n",
        "            result = anderson(valores_imputados)\n",
        "\n",
        "            resultados_ad[col] = {\n",
        "                'statistic': result.statistic,\n",
        "                'critical_values': result.critical_values,\n",
        "                'significance_level': result.significance_level,\n",
        "                'es_normal': result.statistic < result.critical_values[2]  # 5% nivel\n",
        "            }\n",
        "\n",
        "        return resultados_ad\n",
        "\n",
        "    def comparar_metodos(self, columnas: List[str],\n",
        "                        porcentaje_nulos: float = 20.0,\n",
        "                        metodos: List[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compara todos los m√©todos de imputaci√≥n con tests estad√≠sticos.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con comparaci√≥n detallada\n",
        "        \"\"\"\n",
        "        if metodos is None:\n",
        "            metodos = ['knn', 'mice', 'regresion', 'interpolacion']\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üî¨ COMPARACI√ìN ESTAD√çSTICA DE M√âTODOS DE IMPUTACI√ìN\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Introducir nulos controlados\n",
        "        df_con_nulos = self.introducir_nulos_controlados(columnas, porcentaje_nulos)\n",
        "        mascara_nulos = df_con_nulos[columnas].isna()\n",
        "\n",
        "        comparacion = []\n",
        "\n",
        "        for metodo in metodos:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Evaluando m√©todo: {metodo.upper()}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            try:\n",
        "                # Aplicar imputaci√≥n\n",
        "                imputador = ImputadorAvanzado(df_con_nulos)\n",
        "\n",
        "                if metodo == 'knn':\n",
        "                    df_imputado = imputador.imputar_knn(columnas, n_neighbors=5)\n",
        "                elif metodo == 'mice':\n",
        "                    df_imputado = imputador.imputar_mice(columnas, max_iter=10)\n",
        "                elif metodo == 'regresion':\n",
        "                    df_imputado = imputador.imputar_regresion(columnas)\n",
        "                elif metodo == 'interpolacion':\n",
        "                    df_imputado = imputador.imputar_interpolacion(columnas, metodo='linear')\n",
        "\n",
        "                # Calcular m√©tricas de error\n",
        "                metricas = self.calcular_metricas_error(df_imputado, columnas, mascara_nulos)\n",
        "\n",
        "                # Tests estad√≠sticos\n",
        "                ks_test = self.test_kolmogorov_smirnov(df_imputado, columnas)\n",
        "\n",
        "                # Agregar a comparaci√≥n\n",
        "                for col in columnas:\n",
        "                    comparacion.append({\n",
        "                        'M√©todo': metodo.upper(),\n",
        "                        'Columna': col,\n",
        "                        'MAE': metricas[col]['MAE'],\n",
        "                        'RMSE': metricas[col]['RMSE'],\n",
        "                        'MAPE (%)': metricas[col]['MAPE'],\n",
        "                        'R¬≤': metricas[col]['R2'],\n",
        "                        'Bias': metricas[col]['Bias'],\n",
        "                        'KS p-value': ks_test[col]['p_value'],\n",
        "                        'Dist. Similar': '‚úì' if ks_test[col]['son_similares'] else '‚úó'\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error con m√©todo {metodo}: {str(e)}\")\n",
        "\n",
        "        df_comparacion = pd.DataFrame(comparacion)\n",
        "        self.resultados['comparacion'] = df_comparacion\n",
        "\n",
        "        return df_comparacion\n",
        "\n",
        "    def recomendar_mejor_metodo(self, df_comparacion: pd.DataFrame) -> Dict:\n",
        "        \"\"\"\n",
        "        Recomienda el mejor m√©todo bas√°ndose en m√∫ltiples criterios.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üèÜ RECOMENDACI√ìN DE MEJOR M√âTODO\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        recomendaciones = {}\n",
        "\n",
        "        for columna in df_comparacion['Columna'].unique():\n",
        "            df_col = df_comparacion[df_comparacion['Columna'] == columna].copy()\n",
        "\n",
        "            # Normalizar m√©tricas (menor es mejor para MAE, RMSE, MAPE)\n",
        "            df_col['Score_MAE'] = 1 - (df_col['MAE'] - df_col['MAE'].min()) / (df_col['MAE'].max() - df_col['MAE'].min() + 1e-10)\n",
        "            df_col['Score_RMSE'] = 1 - (df_col['RMSE'] - df_col['RMSE'].min()) / (df_col['RMSE'].max() - df_col['RMSE'].min() + 1e-10)\n",
        "            df_col['Score_MAPE'] = 1 - (df_col['MAPE (%)'] - df_col['MAPE (%)'].min()) / (df_col['MAPE (%)'].max() - df_col['MAPE (%)'].min() + 1e-10)\n",
        "\n",
        "            # Mayor es mejor para R¬≤\n",
        "            df_col['Score_R2'] = (df_col['R¬≤'] - df_col['R¬≤'].min()) / (df_col['R¬≤'].max() - df_col['R¬≤'].min() + 1e-10)\n",
        "\n",
        "            # Score compuesto (promedio ponderado)\n",
        "            df_col['Score_Total'] = (\n",
        "                df_col['Score_MAE'] * 0.3 +\n",
        "                df_col['Score_RMSE'] * 0.3 +\n",
        "                df_col['Score_MAPE'] * 0.2 +\n",
        "                df_col['Score_R2'] * 0.2\n",
        "            )\n",
        "\n",
        "            mejor = df_col.loc[df_col['Score_Total'].idxmax()]\n",
        "\n",
        "            recomendaciones[columna] = {\n",
        "                'mejor_metodo': mejor['M√©todo'],\n",
        "                'score_total': mejor['Score_Total'],\n",
        "                'mae': mejor['MAE'],\n",
        "                'rmse': mejor['RMSE'],\n",
        "                'mape': mejor['MAPE (%)'],\n",
        "                'r2': mejor['R¬≤']\n",
        "            }\n",
        "\n",
        "            print(f\"\\nüìä {columna}:\")\n",
        "            print(f\"   üèÖ Mejor m√©todo: {mejor['M√©todo']}\")\n",
        "            print(f\"   üìà Score total: {mejor['Score_Total']:.3f}\")\n",
        "            print(f\"   üìâ MAE: {mejor['MAE']:.4f}\")\n",
        "            print(f\"   üìâ RMSE: {mejor['RMSE']:.4f}\")\n",
        "            print(f\"   üìâ MAPE: {mejor['MAPE (%)']:.2f}%\")\n",
        "            print(f\"   üìà R¬≤: {mejor['R¬≤']:.4f}\")\n",
        "\n",
        "        return recomendaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcuCaPtWhxhI"
      },
      "source": [
        "## 2.2 Funciones de Visualizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRIedQLKhxhI"
      },
      "outputs": [],
      "source": [
        "def visualizar_comparacion_metodos(df_comparacion: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Crea visualizaciones comparativas de los m√©todos de imputaci√≥n.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Comparaci√≥n de M√©todos de Imputaci√≥n', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. MAE por m√©todo\n",
        "    ax1 = axes[0, 0]\n",
        "    df_pivot = df_comparacion.pivot_table(values='MAE', index='Columna', columns='M√©todo')\n",
        "    df_pivot.plot(kind='bar', ax=ax1, width=0.8)\n",
        "    ax1.set_title('MAE (Mean Absolute Error) - Menor es Mejor', fontweight='bold')\n",
        "    ax1.set_ylabel('MAE')\n",
        "    ax1.set_xlabel('Columna')\n",
        "    ax1.legend(title='M√©todo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. RMSE por m√©todo\n",
        "    ax2 = axes[0, 1]\n",
        "    df_pivot = df_comparacion.pivot_table(values='RMSE', index='Columna', columns='M√©todo')\n",
        "    df_pivot.plot(kind='bar', ax=ax2, width=0.8)\n",
        "    ax2.set_title('RMSE (Root Mean Squared Error) - Menor es Mejor', fontweight='bold')\n",
        "    ax2.set_ylabel('RMSE')\n",
        "    ax2.set_xlabel('Columna')\n",
        "    ax2.legend(title='M√©todo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. R¬≤ por m√©todo\n",
        "    ax3 = axes[1, 0]\n",
        "    df_pivot = df_comparacion.pivot_table(values='R¬≤', index='Columna', columns='M√©todo')\n",
        "    df_pivot.plot(kind='bar', ax=ax3, width=0.8)\n",
        "    ax3.set_title('R¬≤ Score - Mayor es Mejor', fontweight='bold')\n",
        "    ax3.set_ylabel('R¬≤')\n",
        "    ax3.set_xlabel('Columna')\n",
        "    ax3.legend(title='M√©todo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax3.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Bueno (0.8)')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. MAPE por m√©todo\n",
        "    ax4 = axes[1, 1]\n",
        "    df_pivot = df_comparacion.pivot_table(values='MAPE (%)', index='Columna', columns='M√©todo')\n",
        "    df_pivot.plot(kind='bar', ax=ax4, width=0.8)\n",
        "    ax4.set_title('MAPE (%) - Menor es Mejor', fontweight='bold')\n",
        "    ax4.set_ylabel('MAPE (%)')\n",
        "    ax4.set_xlabel('Columna')\n",
        "    ax4.legend(title='M√©todo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualizar_distribucion_errores(df_original, df_imputado, columna, metodo, mascara_nulos):\n",
        "    \"\"\"\n",
        "    Visualiza la distribuci√≥n de errores para un m√©todo espec√≠fico.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    fig.suptitle(f'An√°lisis de Errores: {metodo.upper()} - Columna: {columna}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Obtener valores\n",
        "    valores_originales = df_original.loc[mascara_nulos[columna], columna]\n",
        "    valores_imputados = df_imputado.loc[mascara_nulos[columna], columna]\n",
        "    errores = valores_imputados - valores_originales\n",
        "\n",
        "    # 1. Scatter plot: Original vs Imputado\n",
        "    ax1 = axes[0]\n",
        "    ax1.scatter(valores_originales, valores_imputados, alpha=0.6, s=50)\n",
        "\n",
        "    # L√≠nea de referencia perfecta\n",
        "    min_val = min(valores_originales.min(), valores_imputados.min())\n",
        "    max_val = max(valores_originales.max(), valores_imputados.max())\n",
        "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfecta predicci√≥n')\n",
        "\n",
        "    ax1.set_xlabel('Valores Originales', fontsize=11)\n",
        "    ax1.set_ylabel('Valores Imputados', fontsize=11)\n",
        "    ax1.set_title('Original vs Imputado', fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Distribuci√≥n de errores\n",
        "    ax2 = axes[1]\n",
        "    ax2.hist(errores, bins=30, edgecolor='black', alpha=0.7)\n",
        "    ax2.axvline(x=0, color='red', linestyle='--', lw=2, label='Error = 0')\n",
        "    ax2.axvline(x=errores.mean(), color='green', linestyle='-', lw=2, label=f'Media = {errores.mean():.2f}')\n",
        "    ax2.set_xlabel('Error (Imputado - Original)', fontsize=11)\n",
        "    ax2.set_ylabel('Frecuencia', fontsize=11)\n",
        "    ax2.set_title('Distribuci√≥n de Errores', fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Residuales\n",
        "    ax3 = axes[2]\n",
        "    ax3.scatter(valores_imputados, errores, alpha=0.6, s=50)\n",
        "    ax3.axhline(y=0, color='red', linestyle='--', lw=2)\n",
        "    ax3.set_xlabel('Valores Imputados', fontsize=11)\n",
        "    ax3.set_ylabel('Residuales', fontsize=11)\n",
        "    ax3.set_title('An√°lisis de Residuales', fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def crear_heatmap_metricas(df_comparacion):\n",
        "    \"\"\"\n",
        "    Crea un heatmap de las m√©tricas por m√©todo.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Heatmap de M√©tricas por M√©todo', fontsize=16, fontweight='bold')\n",
        "\n",
        "    metricas = ['MAE', 'RMSE', 'MAPE (%)', 'R¬≤']\n",
        "\n",
        "    for idx, metrica in enumerate(metricas):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "        pivot = df_comparacion.pivot_table(values=metrica, index='Columna', columns='M√©todo')\n",
        "\n",
        "        # Para R¬≤, mayor es mejor (usar cmap inverso)\n",
        "        if metrica == 'R¬≤':\n",
        "            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax,\n",
        "                       cbar_kws={'label': metrica}, vmin=0, vmax=1)\n",
        "        else:\n",
        "            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn_r', ax=ax,\n",
        "                       cbar_kws={'label': metrica})\n",
        "\n",
        "        ax.set_title(f'{metrica}', fontweight='bold')\n",
        "        ax.set_xlabel('M√©todo')\n",
        "        ax.set_ylabel('Columna')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou6ShzH0hxhN"
      },
      "source": [
        "# üìã PARTE 3: Carga de nuestros datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJpvZ1v1hxhN"
      },
      "source": [
        "## 3.1 Cargar Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQZslu_KhxhN",
        "outputId": "52a6bef0-7e76-4fb4-a929-73891c14bfdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datos cargados\n",
            "Dimensiones: (12575, 11)\n"
          ]
        }
      ],
      "source": [
        "# Cargar tu dataset\n",
        "df_tus_datos = pd.read_csv('/content/retail_store_sales.csv')\n",
        "# df_tus_datos = pd.read_excel('tu_archivo.xlsx')\n",
        "\n",
        "# Para este ejemplo, usaremos el dataset que creamos\n",
        "# df_tus_datos = df.copy()\n",
        "\n",
        "# Introducir algunos nulos artificialmente para demostrar la limpieza\n",
        "# np.random.seed(123)\n",
        "# indices_nulos = np.random.choice(df_tus_datos.index, 80, replace=False)\n",
        "# df_tus_datos.loc[indices_nulos[:30], 'edad'] = np.nan\n",
        "# df_tus_datos.loc[indices_nulos[30:60], 'salario'] = np.nan\n",
        "# df_tus_datos.loc[indices_nulos[60:], 'departamento'] = np.nan\n",
        "\n",
        "print(\"‚úÖ Datos cargados\")\n",
        "print(f\"Dimensiones: {df_tus_datos.shape}\")\n",
        "# print(f\"\\nNulos por columna:\")\n",
        "# print(df_tus_datos.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gey4NBghxhN"
      },
      "source": [
        "## 3.2 An√°lisis Inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fe3tUL-hxhV",
        "outputId": "80948b30-cff0-4155-9ac8-c02b45438f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üîç AN√ÅLISIS DE TIPOS DE VARIABLES\n",
            "======================================================================\n",
            "\n",
            "üìä FECHAS: 11\n",
            "   ‚Ä¢ Transaction ID: √önicos=12575, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Customer ID: √önicos=25, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Category: √önicos=8, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Item: √önicos=200, Nulos=1213 (9.6%)\n",
            "   ‚Ä¢ Price Per Unit: √önicos=25, Nulos=609 (4.8%), Rango=[5.00, 41.00]\n",
            "   ‚Ä¢ Quantity: √önicos=10, Nulos=604 (4.8%), Rango=[1.00, 10.00]\n",
            "   ‚Ä¢ Total Spent: √önicos=227, Nulos=604 (4.8%), Rango=[5.00, 410.00]\n",
            "   ‚Ä¢ Payment Method: √önicos=3, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Location: √önicos=2, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Transaction Date: √önicos=1114, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Discount Applied: √önicos=2, Nulos=4199 (33.4%)\n",
            "\n",
            "======================================================================\n",
            "üìâ AN√ÅLISIS DE DATOS FALTANTES\n",
            "======================================================================\n",
            "\n",
            "üìã Resumen de Datos Faltantes:\n",
            "         columna  n_nulos  pct_nulos              patron   tipo\n",
            "Discount Applied     4199  33.391650 MCAR (posiblemente) fechas\n",
            "            Item     1213   9.646123 MCAR (posiblemente) fechas\n",
            "  Price Per Unit      609   4.842942 MCAR (posiblemente) fechas\n",
            "        Quantity      604   4.803181 MCAR (posiblemente) fechas\n",
            "     Total Spent      604   4.803181  MAR (posiblemente) fechas\n",
            "\n",
            "üî¨ Patr√≥n de Missingness: Probable (baja varianza en distribuci√≥n de nulos)\n",
            "\n",
            "======================================================================\n",
            "üìä RESUMEN GENERAL\n",
            "======================================================================\n",
            "Filas: 12,575\n",
            "Columnas: 11\n",
            "Memoria: 5.62 MB\n"
          ]
        }
      ],
      "source": [
        "# Analizar tus datos\n",
        "analizador = AnalizadorDatos(df_tus_datos)\n",
        "reporte_tus_datos = analizador.generar_reporte_completo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqQuBjoshxhV"
      },
      "source": [
        "## 3.3 Comparar Preservaci√≥n de Datos por M√©todo\n",
        "\n",
        "Compara cu√°ntos datos se preservan con diferentes estrategias de imputaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4qglB2ShxhV",
        "outputId": "6410d04e-b395-4d78-f4d1-4b21f24a0a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üîç COMPARACI√ìN DE PRESERVACI√ìN DE DATOS\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üîç AN√ÅLISIS DE TIPOS DE VARIABLES\n",
            "======================================================================\n",
            "\n",
            "üìä FECHAS: 11\n",
            "   ‚Ä¢ Transaction ID: √önicos=12575, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Customer ID: √önicos=25, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Category: √önicos=8, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Item: √önicos=200, Nulos=1213 (9.6%)\n",
            "   ‚Ä¢ Price Per Unit: √önicos=25, Nulos=609 (4.8%), Rango=[5.00, 41.00]\n",
            "   ‚Ä¢ Quantity: √önicos=10, Nulos=604 (4.8%), Rango=[1.00, 10.00]\n",
            "   ‚Ä¢ Total Spent: √önicos=227, Nulos=604 (4.8%), Rango=[5.00, 410.00]\n",
            "   ‚Ä¢ Payment Method: √önicos=3, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Location: √önicos=2, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Transaction Date: √önicos=1114, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Discount Applied: √önicos=2, Nulos=4199 (33.4%)\n",
            "\n",
            "======================================================================\n",
            "üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "‚úÖ 'Item': 1213 filas eliminadas (9.6% nulos)\n",
            "‚úÖ 'Discount Applied': 3783 filas eliminadas (33.3% nulos)\n",
            "\n",
            "üìä Filas totales eliminadas: 4996 (39.7%)\n",
            "\n",
            "======================================================================\n",
            "üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "‚úÖ 'Item': 1213 filas eliminadas (9.6% nulos)\n",
            "‚úÖ 'Discount Applied': 3783 filas eliminadas (33.3% nulos)\n",
            "\n",
            "üìä Filas totales eliminadas: 4996 (39.7%)\n",
            "\n",
            "======================================================================\n",
            "üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "‚úÖ 'Item': 1213 filas eliminadas (9.6% nulos)\n",
            "‚úÖ 'Discount Applied': 3783 filas eliminadas (33.3% nulos)\n",
            "\n",
            "üìä Filas totales eliminadas: 4996 (39.7%)\n",
            "\n",
            "======================================================================\n",
            "üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "‚úÖ 'Item': 1213 filas eliminadas (9.6% nulos)\n",
            "‚úÖ 'Discount Applied': 3783 filas eliminadas (33.3% nulos)\n",
            "\n",
            "üìä Filas totales eliminadas: 4996 (39.7%)\n",
            "\n",
            "======================================================================\n",
            "üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "‚úÖ 'Item': 1213 filas eliminadas (9.6% nulos)\n",
            "‚úÖ 'Discount Applied': 3783 filas eliminadas (33.3% nulos)\n",
            "\n",
            "üìä Filas totales eliminadas: 4996 (39.7%)\n",
            "\n",
            "======================================================================\n",
            "üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "‚úÖ 'Item': 1213 filas eliminadas (9.6% nulos)\n",
            "‚úÖ 'Discount Applied': 3783 filas eliminadas (33.3% nulos)\n",
            "\n",
            "üìä Filas totales eliminadas: 4996 (39.7%)\n",
            "\n",
            "======================================================================\n",
            "üìä TABLA COMPARATIVA DE PRESERVACI√ìN\n",
            "======================================================================\n",
            "                 Estrategia  Filas Finales % Preservado  Nulos Restantes  Filas Perdidas\n",
            "Sin imputar (solo eliminar)           7579        60.3%                0            4996\n",
            "                  KNN (k=3)           7579        60.3%                0            4996\n",
            "                  KNN (k=5)           7579        60.3%                0            4996\n",
            "                  KNN (k=7)           7579        60.3%                0            4996\n",
            "                       MICE           7579        60.3%                0            4996\n",
            "                  Regresi√≥n           7579        60.3%                0            4996\n",
            "\n",
            "======================================================================\n",
            "üèÜ RECOMENDACI√ìN\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Mejor estrategia: Sin imputar (solo eliminar)\n",
            "   üìà Preserva: 60.3% de los datos\n",
            "   üìâ Nulos restantes: 0\n",
            "\n",
            "üí° Usa este m√©todo en la secci√≥n 4.4 para obtener mejores resultados\n"
          ]
        }
      ],
      "source": [
        "# Comparar diferentes estrategias para ver cu√°l preserva m√°s datos\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üîç COMPARACI√ìN DE PRESERVACI√ìN DE DATOS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Identificar variables\n",
        "analizador_comp = AnalizadorDatos(df_tus_datos)\n",
        "clasificacion_comp = analizador_comp.identificar_tipos_variables()\n",
        "\n",
        "cols_num = (clasificacion_comp['numericas_continuas'] +\n",
        "           clasificacion_comp['numericas_discretas'])\n",
        "cols_num_nulos = [c for c in cols_num if df_tus_datos[c].isna().sum() > 0]\n",
        "\n",
        "estrategias = [\n",
        "    {'nombre': 'Sin imputar (solo eliminar)', 'metodo': None},\n",
        "    {'nombre': 'KNN (k=3)', 'metodo': 'knn', 'params': {'n_neighbors': 3}},\n",
        "    {'nombre': 'KNN (k=5)', 'metodo': 'knn', 'params': {'n_neighbors': 5}},\n",
        "    {'nombre': 'KNN (k=7)', 'metodo': 'knn', 'params': {'n_neighbors': 7}},\n",
        "    {'nombre': 'MICE', 'metodo': 'mice', 'params': {'max_iter': 10}},\n",
        "    {'nombre': 'Regresi√≥n', 'metodo': 'regresion', 'params': {}},\n",
        "]\n",
        "\n",
        "resultados_preservacion = []\n",
        "\n",
        "for estrategia in estrategias:\n",
        "    df_temp = df_tus_datos.copy()\n",
        "\n",
        "    try:\n",
        "        # Aplicar imputaci√≥n si corresponde\n",
        "        if estrategia['metodo'] and len(cols_num_nulos) > 0:\n",
        "            imputador_temp = ImputadorAvanzado(df_temp)\n",
        "\n",
        "            if estrategia['metodo'] == 'knn':\n",
        "                df_temp = imputador_temp.imputar_knn(\n",
        "                    cols_num_nulos,\n",
        "                    n_neighbors=estrategia['params']['n_neighbors']\n",
        "                )\n",
        "            elif estrategia['metodo'] == 'mice':\n",
        "                df_temp = imputador_temp.imputar_mice(\n",
        "                    cols_num_nulos,\n",
        "                    max_iter=estrategia['params']['max_iter']\n",
        "                )\n",
        "            elif estrategia['metodo'] == 'regresion':\n",
        "                df_temp = imputador_temp.imputar_regresion(cols_num_nulos)\n",
        "\n",
        "        # Eliminar nulos categ√≥ricos\n",
        "        imputador_temp = ImputadorAvanzado(df_temp)\n",
        "        df_temp = imputador_temp.eliminar_categoricos_nulos(umbral_porcentaje=50.0)\n",
        "\n",
        "        # Calcular preservaci√≥n\n",
        "        filas_preservadas = len(df_temp)\n",
        "        pct_preservado = (filas_preservadas / len(df_tus_datos)) * 100\n",
        "        nulos_restantes = df_temp.isna().sum().sum()\n",
        "\n",
        "        resultados_preservacion.append({\n",
        "            'Estrategia': estrategia['nombre'],\n",
        "            'Filas Finales': filas_preservadas,\n",
        "            '% Preservado': f\"{pct_preservado:.1f}%\",\n",
        "            'Nulos Restantes': nulos_restantes,\n",
        "            'Filas Perdidas': len(df_tus_datos) - filas_preservadas\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error con {estrategia['nombre']}: {str(e)}\")\n",
        "\n",
        "# Mostrar resultados\n",
        "df_preservacion = pd.DataFrame(resultados_preservacion)\n",
        "df_preservacion = df_preservacion.sort_values('Filas Finales', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä TABLA COMPARATIVA DE PRESERVACI√ìN\")\n",
        "print(\"=\"*70)\n",
        "print(df_preservacion.to_string(index=False))\n",
        "\n",
        "# Recomendaci√≥n\n",
        "mejor = df_preservacion.iloc[0]\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üèÜ RECOMENDACI√ìN\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n‚úÖ Mejor estrategia: {mejor['Estrategia']}\")\n",
        "print(f\"   üìà Preserva: {mejor['% Preservado']} de los datos\")\n",
        "print(f\"   üìâ Nulos restantes: {mejor['Nulos Restantes']}\")\n",
        "print(f\"\\nüí° Usa este m√©todo en la secci√≥n 4.4 para obtener mejores resultados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYruDTerhxhW"
      },
      "source": [
        "## 3.4 Pipeline de Limpieza Paso a Paso\n",
        "\n",
        "**Estrategia**:\n",
        "1. Primero **imputamos** las variables num√©ricas (no perdemos filas)\n",
        "2. Luego **eliminamos** nulos categ√≥ricos (perdemos menos filas)\n",
        "3. Finalmente corregimos fechas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LgNwXpIhxhW",
        "outputId": "c11e5f4d-c572-436f-ffc6-f25f545dd669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PASO 1: IDENTIFICANDO TIPOS DE VARIABLES\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üîç AN√ÅLISIS DE TIPOS DE VARIABLES\n",
            "======================================================================\n",
            "\n",
            "üìä FECHAS: 11\n",
            "   ‚Ä¢ Transaction ID: √önicos=12575, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Customer ID: √önicos=25, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Category: √önicos=8, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Item: √önicos=200, Nulos=1213 (9.6%)\n",
            "   ‚Ä¢ Price Per Unit: √önicos=25, Nulos=609 (4.8%), Rango=[5.00, 41.00]\n",
            "   ‚Ä¢ Quantity: √önicos=10, Nulos=604 (4.8%), Rango=[1.00, 10.00]\n",
            "   ‚Ä¢ Total Spent: √önicos=227, Nulos=604 (4.8%), Rango=[5.00, 410.00]\n",
            "   ‚Ä¢ Payment Method: √önicos=3, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Location: √önicos=2, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Transaction Date: √önicos=1114, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Discount Applied: √önicos=2, Nulos=4199 (33.4%)\n",
            "\n",
            "üìä Variables num√©ricas con nulos: 0\n",
            "üìä Variables categ√≥ricas con nulos: 0\n",
            "\n",
            "üî∏ Estado inicial:\n",
            "   Filas: 12,575\n",
            "   Nulos totales: 7,229\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PASO 1: Identificar variables num√©ricas y categ√≥ricas\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PASO 1: IDENTIFICANDO TIPOS DE VARIABLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "analizador_temp = AnalizadorDatos(df_tus_datos)\n",
        "clasificacion = analizador_temp.identificar_tipos_variables()\n",
        "\n",
        "# Variables num√©ricas\n",
        "cols_numericas = (clasificacion['numericas_continuas'] +\n",
        "                 clasificacion['numericas_discretas'])\n",
        "cols_numericas_con_nulos = [col for col in cols_numericas\n",
        "                            if df_tus_datos[col].isna().sum() > 0]\n",
        "\n",
        "# Variables categ√≥ricas\n",
        "cols_categoricas = (clasificacion['categoricas_nominales'] +\n",
        "                   clasificacion['categoricas_ordinales'])\n",
        "cols_categoricas_con_nulos = [col for col in cols_categoricas\n",
        "                             if df_tus_datos[col].isna().sum() > 0]\n",
        "\n",
        "print(f\"\\nüìä Variables num√©ricas con nulos: {len(cols_numericas_con_nulos)}\")\n",
        "print(f\"üìä Variables categ√≥ricas con nulos: {len(cols_categoricas_con_nulos)}\")\n",
        "\n",
        "# Guardar estado inicial\n",
        "nulos_iniciales = df_tus_datos.isna().sum().sum()\n",
        "filas_iniciales = len(df_tus_datos)\n",
        "\n",
        "print(f\"\\nüî∏ Estado inicial:\")\n",
        "print(f\"   Filas: {filas_iniciales:,}\")\n",
        "print(f\"   Nulos totales: {nulos_iniciales:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKxnYztphxhW",
        "outputId": "c34b0c89-c27b-4a8a-8bca-b8abbbcf58e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PASO 2: IMPUTANDO VARIABLES NUM√âRICAS\n",
            "======================================================================\n",
            "\n",
            "‚úÖ No hay variables num√©ricas con nulos\n",
            "\n",
            "üìä Nulos en variables num√©ricas: 0.0\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PASO 2: IMPUTAR VARIABLES NUM√âRICAS (preserva todas las filas)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PASO 2: IMPUTANDO VARIABLES NUM√âRICAS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_con_numericas_imputadas = df_tus_datos.copy()\n",
        "\n",
        "if len(cols_numericas_con_nulos) > 0:\n",
        "    imputador_numerico = ImputadorAvanzado(df_con_numericas_imputadas)\n",
        "\n",
        "    # Selecciona el mejor m√©todo basado en la comparaci√≥n anterior\n",
        "    # o usa 'knn' como default (generalmente el m√°s balanceado)\n",
        "    df_con_numericas_imputadas = imputador_numerico.imputar_knn(\n",
        "        columnas=cols_numericas_con_nulos,\n",
        "        n_neighbors=5\n",
        "    )\n",
        "\n",
        "    # Alternativas (descomenta la que prefieras):\n",
        "    # df_con_numericas_imputadas = imputador_numerico.imputar_mice(\n",
        "    #     columnas=cols_numericas_con_nulos, max_iter=10)\n",
        "    # df_con_numericas_imputadas = imputador_numerico.imputar_regresion(\n",
        "    #     columnas=cols_numericas_con_nulos)\n",
        "    # df_con_numericas_imputadas = imputador_numerico.imputar_interpolacion(\n",
        "    #     columnas=cols_numericas_con_nulos, metodo='linear')\n",
        "\n",
        "    print(f\"\\n‚úÖ Variables num√©ricas imputadas exitosamente\")\n",
        "    print(f\"‚úÖ Filas preservadas: {len(df_con_numericas_imputadas):,} (100%)\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No hay variables num√©ricas con nulos\")\n",
        "\n",
        "# Verificar nulos restantes en num√©ricas\n",
        "nulos_numericos_restantes = df_con_numericas_imputadas[cols_numericas].isna().sum().sum()\n",
        "print(f\"\\nüìä Nulos en variables num√©ricas: {nulos_numericos_restantes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etadIV5vhxhW",
        "outputId": "2da57f87-02c9-4049-f9a1-f78e8a62d939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PASO 3: LIMPIANDO VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "\n",
            "‚úÖ No hay variables categ√≥ricas con nulos\n",
            "\n",
            "üìä Impacto de limpieza categ√≥rica:\n",
            "   Filas eliminadas: 0 (0.0%)\n",
            "   Filas preservadas: 12,575 (100.0%)\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PASO 3: ELIMINAR NULOS CATEG√ìRICOS (puede eliminar filas)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PASO 3: LIMPIANDO VARIABLES CATEG√ìRICAS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_limpio = df_con_numericas_imputadas.copy()\n",
        "\n",
        "if len(cols_categoricas_con_nulos) > 0:\n",
        "    print(f\"\\nüî∏ Variables categ√≥ricas con nulos: {cols_categoricas_con_nulos}\")\n",
        "    print(f\"\\nNulos por variable categ√≥rica:\")\n",
        "    for col in cols_categoricas_con_nulos:\n",
        "        n_nulos = df_limpio[col].isna().sum()\n",
        "        pct = (n_nulos / len(df_limpio)) * 100\n",
        "        print(f\"   ‚Ä¢ {col}: {n_nulos} ({pct:.1f}%)\")\n",
        "\n",
        "    # Aplicar limpieza de categ√≥ricas\n",
        "    imputador_categorico = ImputadorAvanzado(df_limpio)\n",
        "\n",
        "    # Ajusta el umbral seg√∫n tus necesidades:\n",
        "    # - umbral bajo (30-40%): M√°s estricto, elimina m√°s columnas\n",
        "    # - umbral medio (50%): Balanceado (default)\n",
        "    # - umbral alto (70-80%): M√°s permisivo, conserva m√°s columnas\n",
        "    df_limpio = imputador_categorico.eliminar_categoricos_nulos(\n",
        "        umbral_porcentaje=50.0  # Ajusta este valor seg√∫n tus necesidades\n",
        "    )\n",
        "else:\n",
        "    print(\"\\n‚úÖ No hay variables categ√≥ricas con nulos\")\n",
        "\n",
        "filas_despues_categoricas = len(df_limpio)\n",
        "filas_eliminadas = filas_iniciales - filas_despues_categoricas\n",
        "pct_eliminado = (filas_eliminadas / filas_iniciales) * 100\n",
        "\n",
        "print(f\"\\nüìä Impacto de limpieza categ√≥rica:\")\n",
        "print(f\"   Filas eliminadas: {filas_eliminadas:,} ({pct_eliminado:.1f}%)\")\n",
        "print(f\"   Filas preservadas: {filas_despues_categoricas:,} ({100-pct_eliminado:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yI3K_lGhxhW",
        "outputId": "72ca8300-e38b-4d18-e6c7-6a00a0e47b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PASO 4: CORRIGIENDO FORMATOS DE FECHA\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìÖ CORRECCI√ìN DE FORMATOS DE FECHA\n",
            "======================================================================\n",
            "‚úÖ 'Transaction Date': 12575 fechas convertidas, 0 fallos\n",
            "   Rango: 2022-01-01 00:00:00 a 2025-01-18 00:00:00\n",
            "\n",
            "‚úÖ Fechas corregidas\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PASO 4: CORREGIR FECHAS (opcional)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PASO 4: CORRIGIENDO FORMATOS DE FECHA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(clasificacion['fechas']) > 0:\n",
        "    corrector = CorreccionFormatos(df_limpio)\n",
        "    df_limpio = corrector.corregir_fechas()\n",
        "    print(\"\\n‚úÖ Fechas corregidas\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No hay columnas de fecha para corregir\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtNl51WQhxhW",
        "outputId": "c6ebb03e-5475-49ff-e982-8b685cae2b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üìä RESUMEN FINAL DE LIMPIEZA\n",
            "======================================================================\n",
            "\n",
            "üî∏ DATOS ORIGINALES:\n",
            "   Filas: 12,575\n",
            "   Nulos: 7,229\n",
            "\n",
            "üî∏ DATOS LIMPIOS:\n",
            "   Filas: 12,575\n",
            "   Nulos: 7,229\n",
            "\n",
            "üî∏ CAMBIOS:\n",
            "   Filas eliminadas: 0 (0.0%)\n",
            "   Filas preservadas: 12,575 (100.0%)\n",
            "   Reducci√≥n de nulos: 0 (0.0%)\n",
            "\n",
            "üî∏ NULOS POR COLUMNA (final):\n",
            "Item                1213\n",
            "Price Per Unit       609\n",
            "Quantity             604\n",
            "Total Spent          604\n",
            "Discount Applied    4199\n",
            "dtype: int64\n",
            "\n",
            "======================================================================\n",
            "‚úÖ LIMPIEZA COMPLETADA EXITOSAMENTE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# RESUMEN FINAL DEL PROCESO\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä RESUMEN FINAL DE LIMPIEZA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "nulos_finales = df_limpio.isna().sum().sum()\n",
        "filas_finales = len(df_limpio)\n",
        "\n",
        "print(f\"\\nüî∏ DATOS ORIGINALES:\")\n",
        "print(f\"   Filas: {filas_iniciales:,}\")\n",
        "print(f\"   Nulos: {nulos_iniciales:,}\")\n",
        "\n",
        "print(f\"\\nüî∏ DATOS LIMPIOS:\")\n",
        "print(f\"   Filas: {filas_finales:,}\")\n",
        "print(f\"   Nulos: {nulos_finales:,}\")\n",
        "\n",
        "print(f\"\\nüî∏ CAMBIOS:\")\n",
        "print(f\"   Filas eliminadas: {filas_iniciales - filas_finales:,} ({((filas_iniciales - filas_finales) / filas_iniciales * 100):.1f}%)\")\n",
        "print(f\"   Filas preservadas: {filas_finales:,} ({(filas_finales / filas_iniciales * 100):.1f}%)\")\n",
        "print(f\"   Reducci√≥n de nulos: {nulos_iniciales - nulos_finales:,} ({((nulos_iniciales - nulos_finales) / max(nulos_iniciales, 1) * 100):.1f}%)\")\n",
        "\n",
        "print(f\"\\nüî∏ NULOS POR COLUMNA (final):\")\n",
        "nulos_por_col = df_limpio.isna().sum()\n",
        "if nulos_por_col.sum() > 0:\n",
        "    print(nulos_por_col[nulos_por_col > 0])\n",
        "else:\n",
        "    print(\"   ‚úÖ No hay nulos restantes\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ LIMPIEZA COMPLETADA EXITOSAMENTE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzwPxRmehxhW"
      },
      "source": [
        "## 3.4b Alternativa: Pipeline Autom√°tico con Control de Imputaci√≥n (Opcional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oyG8HCfhxhX",
        "outputId": "b34df4dd-291e-46f8-c4b2-aef8b10a9c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ INICIANDO PIPELINE DE LIMPIEZA COMPLETO\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üîç AN√ÅLISIS DE TIPOS DE VARIABLES\n",
            "======================================================================\n",
            "\n",
            "üìä FECHAS: 11\n",
            "   ‚Ä¢ Transaction ID: √önicos=12575, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Customer ID: √önicos=25, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Category: √önicos=8, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Item: √önicos=200, Nulos=1213 (9.6%)\n",
            "   ‚Ä¢ Price Per Unit: √önicos=25, Nulos=609 (4.8%), Rango=[5.00, 41.00]\n",
            "   ‚Ä¢ Quantity: √önicos=10, Nulos=604 (4.8%), Rango=[1.00, 10.00]\n",
            "   ‚Ä¢ Total Spent: √önicos=227, Nulos=604 (4.8%), Rango=[5.00, 410.00]\n",
            "   ‚Ä¢ Payment Method: √önicos=3, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Location: √önicos=2, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Transaction Date: √önicos=1114, Nulos=0 (0.0%)\n",
            "   ‚Ä¢ Discount Applied: √önicos=2, Nulos=4199 (33.4%)\n",
            "\n",
            "======================================================================\n",
            "üìâ AN√ÅLISIS DE DATOS FALTANTES\n",
            "======================================================================\n",
            "\n",
            "üìã Resumen de Datos Faltantes:\n",
            "         columna  n_nulos  pct_nulos              patron   tipo\n",
            "Discount Applied     4199  33.391650 MCAR (posiblemente) fechas\n",
            "            Item     1213   9.646123 MCAR (posiblemente) fechas\n",
            "  Price Per Unit      609   4.842942 MCAR (posiblemente) fechas\n",
            "        Quantity      604   4.803181 MCAR (posiblemente) fechas\n",
            "     Total Spent      604   4.803181  MAR (posiblemente) fechas\n",
            "\n",
            "üî¨ Patr√≥n de Missingness: Probable (baja varianza en distribuci√≥n de nulos)\n",
            "\n",
            "======================================================================\n",
            "üìä RESUMEN GENERAL\n",
            "======================================================================\n",
            "Filas: 12,575\n",
            "Columnas: 11\n",
            "Memoria: 5.62 MB\n",
            "\n",
            "======================================================================\n",
            "üìÖ CORRECCI√ìN DE FORMATOS DE FECHA\n",
            "======================================================================\n",
            "‚úÖ 'Transaction Date': 12575 fechas convertidas, 0 fallos\n",
            "   Rango: 2022-01-01 00:00:00 a 2025-01-18 00:00:00\n",
            "\n",
            "======================================================================\n",
            "üóëÔ∏è  ELIMINACI√ìN DE NULOS EN VARIABLES CATEG√ìRICAS\n",
            "======================================================================\n",
            "‚úÖ 'Item': 1213 filas eliminadas (9.6% nulos)\n",
            "‚úÖ 'Discount Applied': 3783 filas eliminadas (33.3% nulos)\n",
            "\n",
            "üìä Filas totales eliminadas: 4996 (39.7%)\n",
            "\n",
            "======================================================================\n",
            "üìä RESUMEN FINAL DE LIMPIEZA\n",
            "======================================================================\n",
            "\n",
            "üî∏ Filas originales: 12,575\n",
            "üî∏ Filas finales: 7,579\n",
            "üî∏ Filas eliminadas: 4,996\n",
            "\n",
            "üî∏ Columnas originales: 11\n",
            "üî∏ Columnas finales: 11\n",
            "\n",
            "üî∏ Nulos originales: 7,229\n",
            "üî∏ Nulos finales: 0\n",
            "üî∏ Reducci√≥n de nulos: 100.0%\n",
            "\n",
            "‚úÖ Pipeline de limpieza completado\n",
            "\n",
            "======================================================================\n",
            "‚úÖ PIPELINE AUTOM√ÅTICO COMPLETADO\n",
            "======================================================================\n",
            "\n",
            "El dataset limpio est√° en la variable: df_limpio_auto\n",
            "Puedes usar df_limpio = df_limpio_auto.copy() para continuar\n"
          ]
        }
      ],
      "source": [
        "# Solo ejecuta esta celda si NO ejecutaste la secci√≥n 4.4 paso a paso\n",
        "\n",
        "# Crear una copia del dataset original\n",
        "df_para_pipeline = df_tus_datos.copy()\n",
        "\n",
        "# Crear limpiador\n",
        "limpiador_auto = LimpiadorCompleto(df_para_pipeline)\n",
        "\n",
        "# Pipeline autom√°tico\n",
        "# IMPORTANTE: El pipeline PRIMERO imputa num√©ricas, LUEGO elimina categ√≥ricas\n",
        "df_limpio_auto = limpiador_auto.pipeline_completo(\n",
        "    metodo_imputacion='knn',     # Opciones: 'knn', 'mice', 'regresion', 'interpolacion'\n",
        "    n_neighbors=5,                # Para KNN: n√∫mero de vecinos\n",
        "    # max_iter=10,                # Para MICE: n√∫mero de iteraciones\n",
        "    # metodo_interpolacion='linear', # Para interpolaci√≥n: tipo de interpolaci√≥n\n",
        "    eliminar_nulos_categoricos=True,  # True = elimina filas con nulos categ√≥ricos\n",
        "    corregir_fechas=True,             # True = corrige formatos de fecha\n",
        "    umbral_categoricos=50.0           # % m√°ximo de nulos antes de eliminar columna\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PIPELINE AUTOM√ÅTICO COMPLETADO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nEl dataset limpio est√° en la variable: df_limpio_auto\")\n",
        "print(f\"Puedes usar df_limpio = df_limpio_auto.copy() para continuar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdO0reJshxhX"
      },
      "source": [
        "### 3.5 Validar Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K94VrXqrhxhX",
        "outputId": "d01fb27d-038c-477a-bab0-b47bd3607234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "‚úÖ VALIDACI√ìN DE LIMPIEZA\n",
            "======================================================================\n",
            "\n",
            "Filas antes: 12575\n",
            "Filas despu√©s: 12575\n",
            "Filas eliminadas: 0\n",
            "\n",
            "Nulos antes: 7229\n",
            "Nulos despu√©s: 7229\n",
            "\n",
            "üìä Nulos por columna (despu√©s):\n",
            "Transaction ID         0\n",
            "Customer ID            0\n",
            "Category               0\n",
            "Item                1213\n",
            "Price Per Unit       609\n",
            "Quantity             604\n",
            "Total Spent          604\n",
            "Payment Method         0\n",
            "Location               0\n",
            "Transaction Date       0\n",
            "Discount Applied    4199\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ VALIDACI√ìN DE LIMPIEZA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nFilas antes: {len(df_tus_datos)}\")\n",
        "print(f\"Filas despu√©s: {len(df_limpio)}\")\n",
        "print(f\"Filas eliminadas: {len(df_tus_datos) - len(df_limpio)}\")\n",
        "\n",
        "print(f\"\\nNulos antes: {df_tus_datos.isna().sum().sum()}\")\n",
        "print(f\"Nulos despu√©s: {df_limpio.isna().sum().sum()}\")\n",
        "\n",
        "print(f\"\\nüìä Nulos por columna (despu√©s):\")\n",
        "print(df_limpio.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilS2nB7FhxhX"
      },
      "source": [
        "## 3.6 Guardar Datos Limpios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evhBtdV3hxhX",
        "outputId": "28a48ec8-1b21-40b5-b7b4-c5723cf77b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datos limpios guardados (descomenta las l√≠neas de arriba para guardar)\n"
          ]
        }
      ],
      "source": [
        "# Guardar datos limpios\n",
        "df_limpio.to_csv('datos_limpios.csv', index=False)\n",
        "# df_limpio.to_excel('datos_limpios.xlsx', index=False)\n",
        "\n",
        "print(\"‚úÖ Datos limpios guardados (descomenta las l√≠neas de arriba para guardar)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}